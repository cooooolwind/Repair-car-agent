import re
import sys
from openai import OpenAI
import os
import base64
from pdf2image import convert_from_path
from func_caller import MockFuncCaller

# --- é…ç½® ---
API_KEY = "sk-5eb60c1091ba459aa9246ea714db371c"
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_ID = "qwen3-vl-plus"

AGENT_SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä¿®è½¦æ‹§èºä¸åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æç”¨æˆ·çš„è¯·æ±‚ï¼ˆå¯èƒ½åŒ…å«å›¾ç‰‡å’Œpdfï¼‰ï¼Œå¹¶ä½¿ç”¨å¯ç”¨å·¥å…·ä¸€æ­¥æ­¥åœ°è§£å†³é—®é¢˜ã€‚

# å¯ç”¨å·¥å…·:
- `get_point()`: æ¨¡æ‹Ÿè§†è§‰ç³»ç»Ÿï¼Œè¿”å›èºä¸æœ‰å‡ ä¸ªã€‚
- `goto_poi(name: str)`: ç§»åŠ¨ç»´ä¿®å°è½¦åˆ°æŒ‡å®šçš„å¯¹åº”nameåœ°ç‚¹ï¼Œæ¯ä¸€ä¸ªåœ°ç‚¹ä¸Šæœ‰ä¸€ä¸ªèºä¸ï¼Œå‚æ•°nameå¯ä»¥æ˜¯2
- `Arm_move(type: str)`: ç§»åŠ¨æœºæ¢°æ‰‹ï¼Œtype=â€˜1â€™è¡¨ç¤ºå‘ä¸Šæ‹§ç´§ï¼Œtype=â€˜0â€™è¡¨ç¤ºå‘ä¸‹æ‹§æ¾ã€‚


# è¡ŒåŠ¨æ ¼å¼:
ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ã€‚é¦–å…ˆæ˜¯ä½ çš„æ€è€ƒè¿‡ç¨‹ï¼Œç„¶åæ˜¯ä½ è¦æ‰§è¡Œçš„å…·ä½“è¡ŒåŠ¨ï¼Œæ¯æ¬¡å›å¤åªè¾“å‡ºä¸€å¯¹Thought-Actionï¼š
Thought: [è¿™é‡Œæ˜¯ä½ çš„æ€è€ƒè¿‡ç¨‹å’Œä¸‹ä¸€æ­¥è®¡åˆ’]
Action: ä½ å†³å®šé‡‡å–çš„è¡ŒåŠ¨ï¼Œå¿…é¡»æ˜¯ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€:
- `function_name(arg_name="arg_value")`:è°ƒç”¨ä¸€ä¸ªå¯ç”¨å·¥å…·ã€‚
- `Finish[æœ€ç»ˆç­”æ¡ˆ]`:å½“ä½ è®¤ä¸ºå·²ç»è·å¾—æœ€ç»ˆç­”æ¡ˆæ—¶ã€‚
- å½“ä½ æ”¶é›†åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œèƒ½å¤Ÿå›ç­”ç”¨æˆ·çš„æœ€ç»ˆé—®é¢˜æ—¶ï¼Œä½ å¿…é¡»åœ¨Action:å­—æ®µåä½¿ç”¨ Finish[æœ€ç»ˆç­”æ¡ˆ] æ¥è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚

è¯·å¼€å§‹å§ï¼
"""

func_caller = MockFuncCaller()

# --- å·¥å…·å‡½æ•° ---
def get_point() -> str:
    return func_caller.get_point()

def goto_poi(name: str) -> str:
    return func_caller.goto_poi(name)

def Arm_move(type: str) -> str:
    return func_caller.arm_move(type)

available_tools = {
    "get_point": get_point,
    "Arm_move": Arm_move,
    "goto_poi":goto_poi,
}


def convert_pdf_to_image(pdf_path):
    try:
        save_dir = "image"
        if not os.path.exists(save_dir): os.makedirs(save_dir)
        images = convert_from_path(pdf_path)
        saved_paths = []
        if images:
            base_name = os.path.splitext(os.path.basename(pdf_path))[0]
            for i, img in enumerate(images):
                save_path = os.path.join(save_dir, f"{base_name}_page_{i}.jpg")
                img.save(save_path, 'JPEG')
                saved_paths.append(save_path)
            return saved_paths
        return []
    except Exception as e:
        print(f"âŒ PDF è½¬æ¢å¤±è´¥: {e}")
        return []


def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')


class OpenAICompatibleClient:
    def __init__(self, model, api_key, base_url):
        self.model = model
        self.client = OpenAI(api_key=api_key, base_url=base_url)

    def generate_stream(self, messages):
        try:
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=True,
                stream_options={"include_usage": False}
            )
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        except Exception as e:
            yield f"LLM è°ƒç”¨é”™è¯¯: {e}"


llm = OpenAICompatibleClient(model=MODEL_ID, api_key=API_KEY, base_url=BASE_URL)


def run_agent(user_text, image_paths=None, history_state=None):
    if image_paths:
        if isinstance(image_paths, str):
            target_images = [image_paths]
        elif isinstance(image_paths, list):
            target_images = image_paths
        else:
            target_images = []
    else:
        target_images = []

    current_user_content = [{"type": "text", "text": user_text}]
    for img_path in target_images:
        try:
            base64_image = encode_image(img_path)
            if base64_image:
                current_user_content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}
                })
        except Exception as e:
            print(f"å›¾ç‰‡è¯»å–å¤±è´¥: {e}")

    current_user_message = {'role': 'user', 'content': current_user_content}
    if history_state is None: history_state = []
    messages = [{'role': 'system', 'content': AGENT_SYSTEM_PROMPT}] + history_state + [current_user_message]

    final_answer = ""
    step_count = 0

    while step_count < 10:
        step_count += 1
        llm_output = ""
        buffer = ""  # ğŸŸ¢ æ™ºèƒ½ç¼“å†²åŒº
        yield {"type": "thought_start", "content": ""}

        is_action_detected = False

        # ğŸŸ¢ æ™ºèƒ½æµå¼å¾ªç¯
        for chunk in llm.generate_stream(messages):
            # å°†æ–°å­—ç¬¦åŠ å…¥ buffer
            buffer += chunk

            # å¦‚æœå·²ç»æ£€æµ‹åˆ° Actionï¼Œå‰©ä¸‹çš„å…¨éƒ¨é™é»˜æ¥æ”¶ï¼Œä¸æ¨ç»™å‰ç«¯
            if is_action_detected:
                llm_output += chunk
                continue

            # æ£€æŸ¥ buffer æ˜¯å¦åŒ…å«åœæ­¢è¯ "Action" æˆ– "Finish"
            # è¿™é‡Œæˆ‘ä»¬æ£€æµ‹ "Action" å³å¯ï¼Œä¸ç”¨ç­‰åˆ°å†’å·ï¼Œé˜²æ­¢ Action å•è¯æœ¬èº«æ³„éœ²
            if "Action" in buffer or "Finish" in buffer:
                # æ‰¾åˆ°åœæ­¢è¯çš„ä½ç½®
                stop_index = -1
                if "Action" in buffer:
                    stop_index = buffer.index("Action")
                elif "Finish" in buffer:
                    stop_index = buffer.index("Finish")

                # å°†åœæ­¢è¯ä¹‹å‰çš„å†…å®¹æ¨ç»™å‰ç«¯
                if stop_index > 0:
                    safe_part = buffer[:stop_index]
                    yield {"type": "thought_stream", "content": safe_part}

                # å‰©ä¸‹çš„å†…å®¹å½’å…¥ llm_outputï¼Œå¹¶æ ‡è®°åœæ­¢æµå¼è¾“å‡º
                llm_output += buffer
                is_action_detected = True
                buffer = ""  # æ¸…ç©º buffer

            else:
                # è¿˜æ²¡æœ‰å®Œå…¨æ£€æµ‹åˆ°åœæ­¢è¯ã€‚
                # æ£€æŸ¥ buffer ç»“å°¾æ˜¯å¦å¯èƒ½æ˜¯åœæ­¢è¯çš„å‰ç¼€ (ä¾‹å¦‚ "A", "Ac", "Act"...)
                # åªæœ‰ç¡®è®¤ *ä¸æ˜¯* åœæ­¢è¯å‰ç¼€çš„å†…å®¹ï¼Œæ‰æ¨ç»™å‰ç«¯

                # ç®€å•çš„å¤„ç†æ–¹å¼ï¼šä¿ç•™æœ€å 6 ä¸ªå­—ç¬¦ (Action çš„é•¿åº¦)
                # å¦‚æœ buffer å¾ˆé•¿ï¼Œå°±æŠŠå‰é¢å®‰å…¨çš„æ¨å‡ºå»
                if len(buffer) > 10:
                    safe_part = buffer[:-10]
                    yield {"type": "thought_stream", "content": safe_part}
                    buffer = buffer[-10:]
                    llm_output += safe_part
                else:
                    # buffer å¤ªçŸ­ï¼Œå¯èƒ½æ­£åœ¨ç”Ÿæˆ "Action"ï¼Œå…ˆæ‰£ä½ä¸å‘
                    pass

        # å¾ªç¯ç»“æŸï¼ŒæŠŠ buffer é‡Œå‰©ä½™çš„é Action å†…å®¹æ¨å‡ºå»ï¼ˆå¦‚æœ Action æ²¡å‡ºç°ï¼‰
        if not is_action_detected and buffer:
            yield {"type": "thought_stream", "content": buffer}
            llm_output += buffer
        elif is_action_detected:
            # å¦‚æœ detectedï¼Œbuffer å·²ç»è¢«åŠ åˆ° llm_output æˆ–è€…æ¸…ç©ºäº†ï¼Œä¸éœ€è¦é¢å¤–æ“ä½œ
            pass

        # å°†å®Œæ•´çš„è¾“å‡ºåŠ å…¥å†å²
        messages.append({'role': 'assistant', 'content': llm_output})

        # --- è§£æé€»è¾‘ ---

        # 1. ä¼˜å…ˆè§£æ Finish
        finish_match = re.search(r"Finish\[(.*?)\]", llm_output, re.DOTALL)
        if not finish_match:
            finish_match = re.search(r"Action:\s*Finish[:\s]+(.*)", llm_output, re.DOTALL)

        if finish_match:
            final_answer = finish_match.group(1).strip()
            yield {"type": "result", "content": final_answer}
            break

        # 2. è§£æ Action
        action_match = re.search(r"Action:\s*`?([a-zA-Z0-9_]+)\((.*)\)`?", llm_output, re.DOTALL)

        if not action_match:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° Action æ ¼å¼ï¼Œä½†ä¹Ÿæ²¡æœ‰ Finish
            if "Action:" not in llm_output:
                # å¯èƒ½æ˜¯çº¯å¯¹è¯ï¼ŒæŠŠ Thought å»æ‰åç›´æ¥æ˜¾ç¤º
                cleaned = re.sub(r"Thought:.*?(?=\n|$)", "", llm_output, flags=re.DOTALL).strip()
                final_answer = cleaned if cleaned else llm_output
                yield {"type": "result", "content": final_answer}
                break
            else:
                observation = "ç³»ç»Ÿæç¤ºï¼šæ— æ³•è§£æ Action æ ¼å¼ï¼Œè¯·æ£€æŸ¥ã€‚"
        else:
            tool_name = action_match.group(1).strip()
            args_str = action_match.group(2).strip()

            # ğŸŸ¢ å¢å¼ºå‚æ•°è§£æï¼šæ”¯æŒä½ç½®å‚æ•° 200, 150 å’Œå…³é”®å­—å‚æ•° x=200
            kwargs = {}
            args_list = []

            if args_str:
                # ç®€å•ç²—æš´çš„è§£æç­–ç•¥
                try:
                    # å°è¯•æŠŠ "200, 150" split
                    parts = [p.strip() for p in args_str.split(",")]
                    for p in parts:
                        if "=" in p:
                            k, v = p.split("=", 1)
                            kwargs[k.strip()] = v.strip().strip("'\"")
                        else:
                            args_list.append(p.strip().strip("'\""))
                except:
                    pass

            yield {"type": "tool_start", "content": f"{tool_name}({args_str})"}

            if tool_name in available_tools:
                try:
                    # æ··åˆè°ƒç”¨
                    observation = available_tools[tool_name](*args_list, **kwargs)
                except Exception as e:
                    observation = f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {e}"
            else:
                observation = f"é”™è¯¯: å·¥å…· '{tool_name}' ä¸å­˜åœ¨"

        yield {"type": "observation", "content": observation}
        messages.append({'role': 'user', 'content': f"Observation: {observation}"})

    if final_answer:
        new_history = history_state + [current_user_message, {'role': 'assistant', 'content': final_answer}]
        yield {"type": "update_state", "content": new_history}