#!/usr/bin/env python3
import pyrealsense2 as rs
import numpy as np
import cv2
from ultralytics import YOLO
from scipy.spatial.transform import Rotation as R
import sys                                                                                                                                                                                                                  
import time

# sys.path.append('//home/robot/GRCNN')
# sys.path.append('//home/robot/GRCNN/real')
from jaka_Rotate_yc import JAKA_Robot  # ç¡®ä¿è·¯å¾„æ­£ç¡®

# ------------------- 1. åˆå§‹åŒ–ç›¸æœº + YOLO -------------------
W, H = 640, 480
pipeline = rs.pipeline()
config = rs.config()
config.enable_stream(rs.stream.depth, W, H, rs.format.z16, 30)
config.enable_stream(rs.stream.color, W, H, rs.format.bgr8, 30)
profile = pipeline.start(config)
print("âœ… RealSense started")

device = profile.get_device()
depth_sensor = device.first_depth_sensor()
depth_scale = depth_sensor.get_depth_scale()
print(f"Depth scale: {depth_scale:.6f} m/unit")

align = rs.align(rs.stream.color)
color_stream = profile.get_stream(rs.stream.color)
color_intr = color_stream.as_video_stream_profile().get_intrinsics()
fx, fy, cx, cy = color_intr.fx, color_intr.fy, color_intr.ppx, color_intr.ppy
print(f"Using color intrinsics: fx={fx:.2f}, fy={fy:.2f}, cx={cx:.2f}, cy={cy:.2f}")



# ------------------- 2. åˆå§‹åŒ–æœºæ¢°è‡‚ -------------------
tcp_host_ip = '10.5.5.100'
robot = JAKA_Robot(tcp_host_ip)

test_pos = [95.982, 187.535, -152.559, 233.931, -267.333, 214.593]
radians_list = [(angle * np.pi) / 180 for angle in test_pos]
robot.move_j(radians_list)


# ------------------- 3. æ‰‹çœ¼æ ‡å®šç»“æœ -------------------
rotation_matrix = np.array([
    [-0.72960442,-0.47869273,-0.488396],
    [ 0.68372925,-0.52506026,-0.50678006],
    [-0.0138454,-0.70367961,0.71038251]
])
translation_vector = np.array([0.09459109,0.05475265,-0.02552082])

# ------------------- 4. åæ ‡è½¬æ¢å‡½æ•° -------------------
def convert(x, y, z, x1, y1, z1, rx, ry, rz):
    obj_camera_coordinates = np.array([x, y, z])
    end_effector_pose = np.array([x1, y1, z1, rx, ry, rz])

    T_camera_to_end_effector = np.eye(4)
    T_camera_to_end_effector[:3, :3] = rotation_matrix
    T_camera_to_end_effector[:3, 3] = translation_vector

    position = end_effector_pose[:3]
    orientation = R.from_euler('xyz', end_effector_pose[3:], degrees=False).as_matrix()

    T_base_to_end_effector = np.eye(4)
    T_base_to_end_effector[:3, :3] = orientation
    T_base_to_end_effector[:3, 3] = position

    obj_camera_h = np.append(obj_camera_coordinates, [1])
    obj_end_effector_h = T_camera_to_end_effector.dot(obj_camera_h)
    obj_base_h = T_base_to_end_effector.dot(obj_end_effector_h)

    return list(obj_base_h[:3])

# ------------------- 5. å·¥å…·å‡½æ•° -------------------
def is_valid_depth(val, depth_scale):
    if val == 0 or val == 65535:
        return False
    z = val * depth_scale
    return 0.001 <= z <= 3.0  # é™åˆ¶æ·±åº¦èŒƒå›´3ç±³å†…

def compute_xyz(u, v, depth_raw, intr, depth_scale):
    Z = depth_raw * depth_scale
    X = (u - intr.ppx) * Z / intr.fx
    Y = (v - intr.ppy) * Z / intr.fy
    return X, Y, Z

# ------------------- 6. ä¸»å¾ªç¯ main arm_work-------------------

print("âœ… ç³»ç»Ÿå¯åŠ¨å®Œæˆï¼Œè¿›å…¥è‡ªåŠ¨æ‹§èºä¸æ¨¡å¼ï¼ˆæ£€æµ‹â†’æ‰§è¡Œâ†’ç­‰å¾…20sâ†’é‡å¤ï¼‰")
model1 = YOLO("best.pt")
model2 = YOLO("hole2.pt")
type = 1    ## 0 down 1 up
print("âœ… YOLO æ¨¡å‹åŠ è½½å®Œæˆ")
try:
    time.sleep(20)
    while True:
        type = (type+1)%2
        frames = pipeline.wait_for_frames()
        aligned = align.process(frames)
        depth_frame = aligned.get_depth_frame()
        color_frame = aligned.get_color_frame()
        if not depth_frame or not color_frame:
            continue

        color_image = np.asanyarray(color_frame.get_data())
        depth_image = np.asanyarray(depth_frame.get_data())

        # YOLO æ£€æµ‹
        if type==0:
            results = model1(color_image)
        
        if type==1:
            results = model2(color_image)
        
        
        annotated = results[0].plot()

        if len(results[0].boxes) > 0:
            # å–ç½®ä¿¡åº¦æœ€é«˜çš„ç›®æ ‡
            best_box = max(results[0].boxes, key=lambda b: float(b.conf[0]))
            x1, y1, x2, y2 = best_box.xyxy[0].cpu().numpy()
            u = int((x1 + x2) / 2)
            v = int((y1 + y2) / 2)
            conf = float(best_box.conf[0])

            # æ ‡æ³¨æ£€æµ‹ç»“æœ
            cv2.circle(annotated, (u, v), 6, (0, 255, 0), -1)
            cv2.putText(annotated, f"Target ({u},{v}) conf={conf:.2f}",
                        (u-60, v-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)
            cv2.putText(annotated, "Screw detected, preparing execution...", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
            cv2.imshow("YOLO Detection", annotated)
            cv2.waitKey(1)

            print(f"\nğŸŸ¢ æ£€æµ‹åˆ°èºä¸ (conf={conf:.2f})ï¼Œç”»é¢å·²åœç•™ã€‚")
            time.sleep(1)  # åœç•™è®©ä½ çœ‹æ¸…æ¥šç›®æ ‡

            # è·å–æ·±åº¦
            depth_raw = int(depth_image[v, u])
            if not is_valid_depth(depth_raw, depth_scale):
                print(f"âš ï¸ æ— æ•ˆæ·±åº¦ ({u},{v})ï¼Œè·³è¿‡ã€‚")
                time.sleep(1)
                continue

            # ç›¸æœºåæ ‡
            X, Y, Z = compute_xyz(u, v, depth_raw, color_intr, depth_scale)
            x1, y1, z1, rx, ry, rz = robot.get_tcp_position()
            RoboX, RoboY, RoboZ = convert(X, Y, Z, x1, y1, z1, rx, ry, rz)

            print(f"åƒç´ ç‚¹ ({u},{v}) -> ç›¸æœºåæ ‡ ({X:.4f}, {Y:.4f}, {Z:.4f}) m")
            print(f"æœºæ¢°è‡‚åŸºåæ ‡: ({RoboX:.4f}, {RoboY:.4f}, {RoboZ:.4f}) m")

            # æ˜¾ç¤ºâ€œæ­£åœ¨æ‰§è¡Œæ‹§èºä¸â€æç¤º
            exec_display = annotated.copy()
            cv2.putText(exec_display, "Executing screw operation...", (20, 80),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
            cv2.imshow("YOLO Detection", exec_display)
            cv2.waitKey(1)
            # æ‰§è¡ŒåŠ¨ä½œ
            robot.plane_grasp([RoboX, RoboY, RoboZ],type)
            if type==0:
                print("âœ… æ‹§èºä¸å®Œæˆã€‚")
            
            elif type==1:
                print("âœ… upèºä¸å®Œæˆã€‚")
            
            

            # æ‰§è¡Œå®Œæˆï¼Œæ˜¾ç¤ºç­‰å¾…æç¤º
            wait_display = annotated.copy()
            cv2.putText(wait_display, "Waiting 10s before next detection...", (20, 120),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
            cv2.imshow("YOLO Detection", wait_display)
            cv2.waitKey(1)

            # ç­‰å¾…10ç§’å†æ¢å¤æ£€æµ‹
            for i in range(10, 0, -1):
                print(f"â³ ç­‰å¾… {i} ç§’åé‡æ–°å¼€å§‹æ£€æµ‹...", end="\r")
                time.sleep(1)

            print("\nğŸ” ç»§ç»­æ£€æµ‹ä¸­...\n")

        else:
            # æ²¡æ£€æµ‹åˆ°ç›®æ ‡æ—¶æ˜¾ç¤ºå®æ—¶ç”»é¢
            cv2.putText(annotated, "Detecting screws...", (20, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
            cv2.imshow("YOLO Detection", annotated)

        # å…è®¸æ‰‹åŠ¨é€€å‡º
        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("â¹ï¸ ç”¨æˆ·é€€å‡ºç¨‹åºã€‚")
            break

finally:
    pipeline.stop()
    cv2.destroyAllWindows()
    print("âœ… ç³»ç»Ÿå·²åœæ­¢ã€‚")
